{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d24576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4421c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430c497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d41d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a430dc7",
   "metadata": {},
   "source": [
    "# Original function\n",
    "\n",
    "- 깃헙에 있는 함수를 그대로 가져온 것입니다.\n",
    "- 이 스터디 덕분에 rotary position embedding에 대해서도 공부하게 되었는데, 그게 어떻게 구현되었는지 살펴보고자 했습니다.\n",
    "- 원본의 input, output 뿐만 아니라 중간 data shape와 같은 흐름 또한 확인하고자했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7c14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    change sign so the last dimension becomes [-odd, +even]\n",
    "    \"\"\"\n",
    "    x = x.view(x.shape[:-1] + torch.Size((2, x.shape[-1] // 2)))\n",
    "    x1, x2 = x.unbind(dim=-2)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb_pytorch(\n",
    "    t: torch.Tensor,\n",
    "    freqs: torch.Tensor,\n",
    "    tensor_format: str = \"sbhd\",\n",
    "    fused: bool = False,\n",
    "    cu_seqlens: Union[torch.Tensor, None] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply rotary positional embedding tensor to the input tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: torch.Tensor\n",
    "        Input tensor of shape `[s, b, h, d]`, `[s, b, h, d]` or `[t, h, d]`, on which\n",
    "        rotary positional embedding will be applied.\n",
    "    freqs: torch.Tensor\n",
    "        Rotary positional embedding tensor of shape `[s2, 1, 1, d2]` and dtype 'float',\n",
    "        with `s2 >= s` and `d2 <= d`.\n",
    "    fused: bool, default = False\n",
    "        Whether to use a fused applying RoPE implementation.\n",
    "    tensor_format: {'sbhd', 'bshd', 'thd'}, default = 'sbhd'\n",
    "        is `bshd` if `t` is of shape `[bs, seq, ...]`, or `sbhd` if `t` is\n",
    "        of shape `[seq, bs, ...]`. 'thd' is only supported when `fused` is True.\n",
    "    cu_seqlens: torch.Tensor, default = None.\n",
    "        Cumulative sum of sequence lengths in a batch for `t`, with shape [b + 1] and\n",
    "        dtype torch.int32. Only valid when `tensor_format` is 'thd'.\n",
    "    \"\"\"\n",
    "    if fused:\n",
    "        assert (\n",
    "            tensor_format != \"thd\" or cu_seqlens is not None\n",
    "        ), \"cu_seqlens must not be None when tensor_format is 'thd'.\"\n",
    "        return FusedRoPEFunc.apply(t, freqs, tensor_format, cu_seqlens)\n",
    "\n",
    "    assert tensor_format in (\"sbhd\", \"bshd\"), (\n",
    "        \"Only formats `sbhd` or `bshd` are supported for input tensor `t` \"\n",
    "        f\"when fused is False, got {tensor_format}.\"\n",
    "    )\n",
    "\n",
    "    max_seq_len = freqs.shape[0]\n",
    "    cur_seq_len = t.shape[1] if tensor_format == \"bshd\" else t.shape[0]\n",
    "\n",
    "    # Only apply the rotary embeddings up to the sequence length of the running\n",
    "    # input.\n",
    "    assert cur_seq_len <= max_seq_len, (\n",
    "        f\"Rotary Embeddings only supported up to {max_seq_len} sequence length!\"\n",
    "    )\n",
    "    freqs = freqs[:cur_seq_len]\n",
    "    if tensor_format == \"bshd\":\n",
    "        freqs = freqs.transpose(0, 1)  # [seq, 1, 1, dim] -> [1, seq, 1, dim]\n",
    "    # cos/sin first then dtype conversion for better precision\n",
    "    cos_ = torch.cos(freqs).to(t.dtype)\n",
    "    sin_ = torch.sin(freqs).to(t.dtype)\n",
    "\n",
    "    rot_dim = freqs.shape[-1]\n",
    "    # ideally t_pass is empty so rotary pos embedding is applied to all tensor t\n",
    "    t, t_pass = t[..., :rot_dim], t[..., rot_dim:]\n",
    "\n",
    "    # first part is cosine component\n",
    "    # second part is sine component, need to change signs with _rotate_half method\n",
    "    t = (t * cos_) + (_rotate_half(t) * sin_)\n",
    "    return torch.cat((t, t_pass), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583dd15",
   "metadata": {},
   "source": [
    "### Prepare input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9fdf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, b, h, d = 10, 2, 4, 16\n",
    "t = torch.randn(s, b, h, d)\n",
    "\n",
    "### hypothsis. s2 = s, d2 = d/2\n",
    "s2, d2 = s, d//2\n",
    "freqs = torch.randn(s2, 1, 1, d2)\n",
    "\n",
    "tensor_format = \"sbhd\"\n",
    "\n",
    "fused = False\n",
    "cu_seqlens = None\n",
    "\n",
    "# t, freqs, tensor_format, fused, cu_seqlens\n",
    "\n",
    "t = t.to('cuda')\n",
    "freqs = freqs.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377184ee",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8a3ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2, 4, 16]),\n",
       " tensor(-0.0223, device='cuda:0'),\n",
       " tensor(1.0054, device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_result = apply_rotary_pos_emb_pytorch(\n",
    "    t=t,\n",
    "    freqs=freqs,\n",
    "    tensor_format=tensor_format,\n",
    ")\n",
    "\n",
    "original_result.shape, original_result.mean(), original_result.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74996d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1c2182f",
   "metadata": {},
   "source": [
    "# Migrate 'apply_rotary_pos_emb' function to triton\n",
    "- _rotate_half_kernel 함수는 이전 노트북에서 구현했던 것을 그대로 가져왔습니다.\n",
    "- 가급적이면 사용자 입장에서 원본 apply_rotary_pos_emb 함수를 호출하는 것과 동일하게 사용하는게 좋겠다 싶어서, 높은 추상화 레벨의 함수에서 apply_rotary_pos_emb_kernel을 호출하는 식으로 구현하고자 했습니다.\n",
    "- 정말 아쉽지만 작성 도중에 제출 마감 시간이 되어 이 함수는 완성하지 못 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58464491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a765ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _rotate_half_kernel(\n",
    "    output_ptr, input_ptr,\n",
    "    batch_size: tl.constexpr, seq_len: tl.constexpr, head_num: tl.constexpr, d_model: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    idx = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    total_elements = batch_size * seq_len * head_num * d_model\n",
    "    mask = idx < total_elements\n",
    "\n",
    "    dim_idx = idx % d_model\n",
    "    half_dim = d_model // 2\n",
    "    is_second_half = dim_idx >= half_dim\n",
    "\n",
    "    swapped_idx = tl.where(is_second_half, idx - half_dim, idx + half_dim)\n",
    "    data = tl.load(input_ptr + idx, mask=mask)\n",
    "    data_swapped = tl.where(is_second_half, -data, data)\n",
    "\n",
    "    tl.store(output_ptr + swapped_idx, data_swapped, mask=mask)\n",
    "\n",
    "        \n",
    "@triton.jit\n",
    "def apply_rotary_pos_emb_kernel(\n",
    "    t_ptr, freqs_ptr, output_ptr, \n",
    "    s: tl.constexpr, b: tl.constexpr, h: tl.constexpr, d: tl.constexpr, d2: tl.constexpr, \n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    row_idx = tl.program_id(0)\n",
    "    col_idx = tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    freq = tl.load(freqs_ptr + row_idx * d2 + col_idx, mask=col_idx < d2, other=0.0)\n",
    "    \n",
    "    cos_val, sin_val = tl.cos(freq), tl.sin(freq)\n",
    "    \n",
    "    input_val = tl.load(t_ptr + row_idx * h * d + col_idx)\n",
    "    ### TODO: make rotated_input_val using __rotate_half_kernel\n",
    "    rotated_input_val = input_val\n",
    "#     __rotate_half_kernel[(grid_size,)](\n",
    "#         rotated_input_val, input_val, \n",
    "#         batch_size=batch_size, seq_len=seq_len, head_num=head_num, d_model=d_model,\n",
    "#         BLOCK_SIZE=BLOCK_SIZE\n",
    "#     )\n",
    "\n",
    "    rotary_emb = cos_val * input_val + sin_val * rotated_input_val\n",
    "\n",
    "    tl.store(output_ptr + row_idx * h * d + col_idx, rotary_emb, mask=col_idx < d)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb_triton(\n",
    "    t: torch.Tensor,\n",
    "    freqs: torch.Tensor,\n",
    "    tensor_format: str = \"sbhd\",\n",
    "    fused: bool = False,\n",
    "    cu_seqlens: Union[torch.Tensor, None] = None,\n",
    ") -> torch.Tensor:\n",
    "    if fused:\n",
    "        assert (\n",
    "            tensor_format != \"thd\" or cu_seqlens is not None\n",
    "        ), \"cu_seqlens must not be None when tensor_format is 'thd'.\"\n",
    "        return FusedRoPEFunc.apply(t, freqs, tensor_format, cu_seqlens)\n",
    "\n",
    "    assert tensor_format in (\"sbhd\", \"bshd\"), (\n",
    "        \"Only formats `sbhd` or `bshd` are supported for input tensor `t` \"\n",
    "        f\"when fused is False, got {tensor_format}.\"\n",
    "    )\n",
    "    \n",
    "    s, b, h, d = t.shape\n",
    "    cur_seq_len = b if tensor_format == \"bshd\" else s\n",
    "    max_seq_len, _, _, d2 = freqs.shape\n",
    "    \n",
    "    # Only apply the rotary embeddings up to the sequence length of the running\n",
    "    assert cur_seq_len <= max_seq_len, (\n",
    "        f\"Rotary Embeddings only supported up to {max_seq_len} sequence length!\"\n",
    "    )\n",
    "    \n",
    "    freqs = freqs[:cur_seq_len]\n",
    "    if tensor_format == \"bshd\":\n",
    "        freqs = freqs.transpose(0, 1)  # [seq, 1, 1, dim] -> [1, seq, 1, dim]\n",
    "    \n",
    "    output = torch.empty_like(t)\n",
    "    BLOCK_SIZE = d\n",
    "    grid = (s * b * h, )\n",
    "    apply_rotary_pos_emb_kernel[grid](\n",
    "        output, t, freqs, \n",
    "        s, b, h, d, d2, \n",
    "        BLOCK_SIZE=BLOCK_SIZE\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0aaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df778041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2, 4, 16]),\n",
       " tensor(0.0003, device='cuda:0'),\n",
       " tensor(0.9146, device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_result = apply_rotary_pos_emb_triton(\n",
    "    t=t,\n",
    "    freqs=freqs,\n",
    "    tensor_format=tensor_format,\n",
    ")\n",
    "\n",
    "triton_result.shape, triton_result.mean(), triton_result.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230cc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c2b4f25",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 1280 / 1280 (100.0%)\nGreatest absolute difference: 4.691768646240234 at index (8, 0, 1, 6) (up to 1e-05 allowed)\nGreatest relative difference: inf at index (1, 0, 0, 0) (up to 1.3e-06 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriton_result\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data-ssd/users/dev_00/miniconda3/envs/ac_study/lib/python3.10/site-packages/torch/testing/_comparison.py:1520\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1498\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1499\u001b[0m     actual,\n\u001b[1;32m   1500\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1516\u001b[0m )\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 1280 / 1280 (100.0%)\nGreatest absolute difference: 4.691768646240234 at index (8, 0, 1, 6) (up to 1e-05 allowed)\nGreatest relative difference: inf at index (1, 0, 0, 0) (up to 1.3e-06 allowed)"
     ]
    }
   ],
   "source": [
    "torch.testing.assert_close(original_result, triton_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257610f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c02ab879",
   "metadata": {},
   "source": [
    "- 스터디 모집 문제에 cuda kernel로 구현한 것까지 첨부해주신 것으로 보아, 해당 코드를 참고해 triton에서 low하게 구현하여 성능 향상을 하는 결과물을 제출하는 것이 목표인 듯 합니다.\n",
    "- 사실 저에게 2~3일로는 이 과제가 쉽지 않을 것 같았지만, 정말 매력적인 분야라 꼭 공부하고 싶었고, 그냥 포기하면 후회할 것 같아서 부딫혀보게 되었습니다.\n",
    "- 제 한계가 고작 이 정도인가 싶어서 자괴감도 들고, 최선을 다 했으니 한편으로는 후련한 복잡한 심정입니다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d753342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
